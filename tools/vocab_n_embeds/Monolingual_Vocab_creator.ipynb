{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "devanagari_word_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOamUR6oCYvh"
      },
      "source": [
        "Used for ceating a cleanset of Monolingual words to used for sanitization of predictions.\n",
        "\n",
        "Data can be of text corpus or word list, and script cleans words with diffenrent language unicodes.\n",
        "Objective is distillas much as possible to purest words yest maintaining a larger word set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZaCvBYQIFBy"
      },
      "source": [
        "scripts = {'ऄ','अ', 'आ', 'इ', 'ई', 'उ', 'ऊ','ऍ', 'ऎ', 'ए', 'ऐ',\n",
        "    'ऑ', 'ऒ', 'ओ', 'औ','ऋ','ॠ','ऌ','ॡ','ॲ', 'ॐ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण',\n",
        "    'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल',\n",
        "    'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़',\n",
        "    '्', 'ा', 'ि', 'ी', 'ु', 'ू', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ',\n",
        "    'ृ', 'ॄ', 'ॢ', 'ॣ', 'ँ', 'ं', 'ः', '़', '॑',  'ऽ',\n",
        "\n",
        "    chr(0x200c), # ZeroWidth-NonJoiner U+200c\n",
        "    chr(0x200d), # ZeroWidthJoiner U+200d\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y-0DLyblyUw"
      },
      "source": [
        "## Removal Set\n",
        "\n",
        "Spl_puncs = [chr(i) for i in range(0x2000,0x208F) ]\n",
        "Latin_chars = [chr(i) for i in range(0x0020,0x007F)]\n",
        "Latin_supli_exten = [chr(i) for i in range(0x00A1,0x017F)]\n",
        "dgri_num = [chr(i) for i in range(0x0966, 0x0970)]\n",
        "Other_chars = [\"\\n\",'×', \"॥\", '।', '॰']\n",
        "\n",
        "remove_chars_set = Latin_chars + Spl_puncs + Latin_supli_exten + dgri_num + Other_chars \n",
        "print(remove_chars_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSdwoofkLfXd"
      },
      "source": [
        "# From **Mono Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDc0m9xBmA6z"
      },
      "source": [
        "## Reading Files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0WjivK_cto"
      },
      "source": [
        "##Directly from TXT\n",
        "with open(\"/content/monolingual/monolingual.hi\") as f:\n",
        "    raw_data = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ4ACzXdnhav"
      },
      "source": [
        "## Cleaning the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vn4HzL8pvNl"
      },
      "source": [
        "clean_data = raw_data\n",
        "for c in remove_chars_set:\n",
        "    clean_data = clean_data.replace(c, \" \")\n",
        "\n",
        "word_tks = clean_data.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKIPv5RFsLra"
      },
      "source": [
        "#print(word_tks) \n",
        "print(len(word_tks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_s-bpY7sPEx"
      },
      "source": [
        "word_dict = dict()\n",
        "for w in word_tks:\n",
        "    v = word_dict.get(w)\n",
        "    if v:\n",
        "        word_dict[w] = v+1\n",
        "    else:\n",
        "        word_dict[w] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuL8X3fyAmo"
      },
      "source": [
        "# print(word_dict)\n",
        "print(len(word_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4OKSqBDH76B"
      },
      "source": [
        "## Check if the characters are devanagari\n",
        "new_set = set(word_dict)\n",
        "removal_word_set = set()\n",
        "for n in new_set:\n",
        "    for c in set(n):\n",
        "        if c in scripts:\n",
        "            continue\n",
        "        else:\n",
        "            print(n)\n",
        "            removal_word_set.add(n)\n",
        "            break\n",
        "print(\"Removal set\", len(removal_word_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "361zysJNdQt2"
      },
      "source": [
        "for k in removal_word_set:\n",
        "    word_dict.pop(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr_O_wyJWKJJ"
      },
      "source": [
        "## Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXLKGnxUyGG7"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "data_items = word_dict.items()\n",
        "# data_list = list(data_items)\n",
        "df = pd.DataFrame(data_items, columns =[\"WORD\", \"FREQ\"] )\n",
        "df = df.sort_values([\"FREQ\"], ascending= False, ignore_index =True)\n",
        "\n",
        "df.to_csv('out_data.csv', index=False, quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcc_pb6aLqwy"
      },
      "source": [
        "# From **Vocab Frequency**\n",
        "\n",
        "AI4Bharat based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daTk_U5dML8V"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVpW9nZ3UIAm"
      },
      "source": [
        "with open(\"/content/mr.vocabfreq.tsv\") as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "## Take only Frequency greater than 1\n",
        "import re\n",
        "word_list = []\n",
        "for d in data:\n",
        "    y = re.split(\"\\t|\\n\", d)\n",
        "    if int(y[1]) > 1:\n",
        "        word_list.append(y[0]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIVWTZeIMebD"
      },
      "source": [
        "## Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rPExzHiMg3X"
      },
      "source": [
        "new_set = set(word_list)\n",
        "removal_word_set = set()\n",
        "for n in new_set:\n",
        "    for c in set(n):\n",
        "        if c in scripts:\n",
        "            continue\n",
        "        else:\n",
        "            print(n)\n",
        "            removal_word_set.add(n)\n",
        "            break\n",
        "print(\"Removal set\", len(removal_word_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kSgIKYiMuTT"
      },
      "source": [
        "for k in removal_word_set:\n",
        "    word_list.remove(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8JLLvx9Mbtj"
      },
      "source": [
        "##Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjjylJXH1wLW"
      },
      "source": [
        "print(len(word_list))\n",
        "\n",
        "word_list = sorted(word_list)\n",
        "import json\n",
        "with open(\"out_words.json\" ,\"w\", encoding = \"utf-8\") as f:\n",
        "    json.dump(word_list, f, ensure_ascii=False, indent=4, sort_keys=True,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJpt1LatWSQV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}